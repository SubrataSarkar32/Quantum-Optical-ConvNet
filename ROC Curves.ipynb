{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "torch.Size([8, 784])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pylab as plt\n",
    "from optical_nn import *\n",
    "import complex_torch_var as ct\n",
    "from mnist import *\n",
    "import os\n",
    "from time import time\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from default_params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_complex(f=F_COMPLEX_TRAIN):\n",
    "    print(f)\n",
    "    net = mnist_complex()\n",
    "    print(net)\n",
    "    net.load_state_dict(th.load(f, map_location=DEVICE))\n",
    "    return net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn(f=F_CNN_TRAIN):\n",
    "    print(f)\n",
    "    net = Net()\n",
    "    print(net)\n",
    "    net.load_state_dict(th.load(f, map_location=DEVICE))\n",
    "    return net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grid(f=os.path.join(DIR_TRAINED_MODELS, 'grid_net_cd2.pth'), rand_S=True, report_acc=True):\n",
    "    if f is None:\n",
    "        f = F_GRID_TRAIN if rand_S else F_GRID_ORD_TRAIN\n",
    "    net = mnist_ONN()\n",
    "    net.load_state_dict(th.load(f, map_location=DEVICE))\n",
    "    acc, confusion_matrix = get_acc(net)\n",
    "    print(f'GridNetOrdered loaded from {f} with accuracy {acc}.')\n",
    "    print(confusion_matrix)\n",
    "    return net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridNetOrdered loaded from E:\\nit dgp\\2nd sem\\Quantum-Optical-ConvNet\\trained_models\\grid_net_cd2.pth with accuracy 0.747757613658905.\n",
      "[[3271  133]\n",
      " [1020  147]]\n",
      "NoisySequential(\n",
      "  (0): Linear(\n",
      "    (VH): Unitary()\n",
      "    (S): Diagonal()\n",
      "    (U): Unitary()\n",
      "  )\n",
      "  (1): ModNonlinearity(\n",
      "    (f): ShiftedSoftplus()\n",
      "  )\n",
      "  (2): Linear(\n",
      "    (VH): Unitary()\n",
      "    (S): Diagonal()\n",
      "    (U): Unitary()\n",
      "  )\n",
      "  (3): ModNonlinearity(\n",
      "    (f): ShiftedSoftplus()\n",
      "  )\n",
      "  (4): Linear(\n",
      "    (VH): Unitary()\n",
      "    (S): Diagonal()\n",
      "    (U): Unitary()\n",
      "  )\n",
      "  (5): ComplexNorm()\n",
      "  (6): LogSoftmax(dim=1)\n",
      ")\n",
      "torch.Size([8, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE), target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 16\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m Y_pred\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(target)\n",
      "File \u001b[1;32mG:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mG:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mG:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\nit dgp\\2nd sem\\Quantum-Optical-ConvNet\\optical_nn.py:957\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;66;03m#Make X a \"complex vector\" if not already\u001b[39;00m\n\u001b[1;32m--> 957\u001b[0m     N, D \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_in:\n\u001b[0;32m    959\u001b[0m         X \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mmake_batched_vec(X)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import math\n",
    "\n",
    "model = load_grid(f=os.path.join(DIR_TRAINED_MODELS, 'grid_net_cd2.pth'))\n",
    "print(model)\n",
    "for data, target in mnist_test_loader(train=False, batch_size=100, shuffle=False):\n",
    "    continue\n",
    "#data = data.view(-1, 28**2)\n",
    "data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "print(data.shape)\n",
    "Y_pred = model(data)\n",
    "out = Y_pred.data.max(1, keepdim=True)[1]\n",
    "print(target)\n",
    "print(out)\n",
    "#print(get_acc(model))\n",
    "Y_pred = torch.exp(Y_pred)\n",
    "Y_pred.detach()\n",
    "Y_real = torch.zeros((8, 2), requires_grad = False)\n",
    "Y_pred = Y_pred.detach().numpy()\n",
    "print(out.eq(target.data.view_as(out)).sum())\n",
    "for i in range(8):\n",
    "    Y_real[i][target[i]] = 1\n",
    "# Plot linewidth.\n",
    "Y_real = Y_real.numpy()\n",
    "lw = 2\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    temp = []\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_real[:, i], Y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_real.ravel(), Y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(2)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(2):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= 2\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "\n",
    "\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot micro/macro ROC curves\n",
    "plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot class ROC curves\n",
    "plt.figure(2)\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue','deeppink','navy','yellow','purple','gray','red','green','brown','blue'])\n",
    "for i, color in zip(range(2), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='Class {0} (AUC = {1:0.6f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.01, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('CNN_ROC.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
